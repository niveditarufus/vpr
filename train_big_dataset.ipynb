{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4a9a2e-dbd7-450e-9c87-1eec9f05e508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import open_clip\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision as tv\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from timm import optim\n",
    "from torch import linalg\n",
    "from torch.nn import functional as F\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f21ccca-af25-4f1e-af05-4fda29300300",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    name = \"vit_224_v3\"\n",
    "    root_dir = \"/home/luffy/workspace/vpr/data\"\n",
    "    num_models_save = 20\n",
    "    lr_model = 2e-7\n",
    "    lr_fc = 1e-4\n",
    "    weight_decay = 1e-2\n",
    "    epochs = 25\n",
    "    warmup_epochs = 1\n",
    "    # start_ema_epoch = 5\n",
    "    model_freeze_epochs = 0\n",
    "    batch_size = 24\n",
    "    img_size = 224\n",
    "    scheduler = \"cos\"  # could be 'cos' or 'step'\n",
    "    num_workers = 8\n",
    "    num_classes = 14087\n",
    "    embedding_size = 768\n",
    "    precision = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42bc76eb-bf9a-471d-9336-1f2572c65d87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_aug():\n",
    "    train_augs = tv.transforms.Compose(\n",
    "        [\n",
    "            tv.transforms.Resize((config.img_size, config.img_size)),\n",
    "            # tv.transforms.RandomResizedCrop((config.img_size, config.img_size)),\n",
    "            tv.transforms.RandomVerticalFlip(),\n",
    "            tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.RandomApply(\n",
    "                [tv.transforms.RandomRotation(degrees=90)], p=0.3\n",
    "            ),\n",
    "            tv.transforms.RandomApply(\n",
    "                [\n",
    "                    tv.transforms.ColorJitter(brightness=0.2, hue=0.3),\n",
    "                ],\n",
    "                p=0.2,\n",
    "            ),\n",
    "            # tv.transforms.RandomApply([tv.transforms.RandAugment()], p=0.3),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "    return train_augs\n",
    "\n",
    "\n",
    "def get_val_aug():\n",
    "    val_augs = tv.transforms.Compose(\n",
    "        [\n",
    "            tv.transforms.Resize((config.img_size, config.img_size)),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "    return val_augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6c01ed-c10e-4f0e-9db3-baac7c4c2177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_image(image_file):\n",
    "    img = cv2.imread(image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Failed to read {}\".format(image_file))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fc203b-5bb1-4c1d-902c-a22ad0ec8c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BigDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self, root, annotation_file, transforms, is_inference=False\n",
    "    ):\n",
    "        self.root = root\n",
    "        self.imlist = pd.read_csv(annotation_file)\n",
    "        self.transforms = transforms\n",
    "        self.is_inference = is_inference\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cv2.setNumThreads(6)\n",
    "        if self.is_inference:\n",
    "            impath, _= self.imlist.iloc[index]\n",
    "        else:\n",
    "            impath, target = self.imlist.iloc[index]\n",
    "\n",
    "        full_imname = os.path.join(self.root, impath)\n",
    "        img = read_image(full_imname)\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.transforms(img)\n",
    "\n",
    "        if self.is_inference:\n",
    "            return img\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf47d1a-50ba-4542-bfd3-e142913459ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataloaders():\n",
    "    print(\"Preparing train reader...\")\n",
    "    train_dataset = BigDataset(\n",
    "        root=os.path.join(config.root_dir),\n",
    "        annotation_file=os.path.join(config.root_dir, \"final_data_224/final_data_224.csv\"),\n",
    "        transforms=get_train_aug(),\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "    print(\"Preparing valid reader...\")\n",
    "    val_dataset = BigDataset(\n",
    "        root=os.path.join(config.root_dir),\n",
    "        annotation_file=os.path.join(config.root_dir, \"final_data_224/final_data_224.csv\"),\n",
    "        transforms=get_val_aug(),\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=config.num_workers,\n",
    "        drop_last=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    print(\"Done.\")\n",
    "\n",
    "    return train_loader, valid_loader, train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a408b758-4957-40e9-bdce-e0f25cf51e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    def __init__(self, cin, cout, s=30, m=0.3):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.sin_m = torch.sin(torch.tensor(m))\n",
    "        self.cos_m = torch.cos(torch.tensor(m))\n",
    "        self.cout = cout\n",
    "        self.fc = nn.Linear(cin, cout, bias=False)\n",
    "\n",
    "    def forward(self, x, label=None):\n",
    "        w_L2 = linalg.norm(self.fc.weight.detach(), dim=1, keepdim=True).T\n",
    "        x_L2 = linalg.norm(x, dim=1, keepdim=True)\n",
    "        cos = self.fc(x) / (x_L2 * w_L2)\n",
    "        if label is not None:\n",
    "            sin_m, cos_m = self.sin_m, self.cos_m\n",
    "            one_hot = F.one_hot(label, num_classes=self.cout)\n",
    "            sin = (1 - cos**2) ** 0.5\n",
    "            angle_sum = cos * cos_m - sin * sin_m\n",
    "            cos = angle_sum * one_hot + cos * (1 - one_hot)\n",
    "            cos = cos * self.s\n",
    "        return cos\n",
    "\n",
    "\n",
    "class Classifier_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier_model, self).__init__()\n",
    "        self.model = open_clip.create_model_and_transforms(\n",
    "            \"ViT-L-14\", pretrained=\"laion2b_s32b_b82k\"\n",
    "        )[0].visual\n",
    "        self.fc = ArcFace(config.embedding_size, config.num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.model(x)\n",
    "        x = self.fc(x, labels)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelEmaV2(torch.nn.Module):\n",
    "    def __init__(self, model, decay=0.9995, device=None):\n",
    "        super(ModelEmaV2, self).__init__()\n",
    "        # make a copy of the model for accumulating moving average of weights\n",
    "        self.module = deepcopy(model)\n",
    "        self.module.eval()\n",
    "        self.decay = decay\n",
    "        self.device = device  # perform ema on different device from model if set\n",
    "        if self.device is not None:\n",
    "            self.module.to(device=device)\n",
    "\n",
    "    def _update(self, model, update_fn):\n",
    "        with torch.no_grad():\n",
    "            for ema_v, model_v in zip(\n",
    "                self.module.state_dict().values(), model.state_dict().values()\n",
    "            ):\n",
    "                if self.device is not None:\n",
    "                    model_v = model_v.to(device=self.device)\n",
    "                ema_v.copy_(update_fn(ema_v, model_v))\n",
    "\n",
    "    def update(self, model):\n",
    "        self._update(\n",
    "            model, update_fn=lambda e, m: self.decay * e + (1.0 - self.decay) * m\n",
    "        )\n",
    "\n",
    "    def set(self, model):\n",
    "        self._update(model, update_fn=lambda e, m: m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d347161e-d5e4-4572-aef3-c67aa9d125e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VPRModule(pl.LightningModule):\n",
    "    def __init__(self, num_train_steps):\n",
    "        super().__init__()\n",
    "        self.model = Classifier_model()\n",
    "        # self.model_ema = ''\n",
    "        self.loss_module = nn.CrossEntropyLoss()\n",
    "        self.num_train_steps = num_train_steps\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        return self.model(img, labels)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\"params\": self.model.model.parameters(), \"lr\": config.lr_model},\n",
    "                {\"params\": self.model.fc.parameters(), \"lr\": config.lr_fc},\n",
    "            ],\n",
    "            weight_decay=config.weight_decay,\n",
    "        )\n",
    "        # self.optimizer = optim.lion.Lion(\n",
    "        #    [\n",
    "        #        {\"params\": self.model.model.parameters(), \"lr\": config.lr_model},\n",
    "        #        {\"params\": self.model.fc.parameters(), \"lr\": config.lr_fc},\n",
    "        #    ],\n",
    "        #    weight_decay=config.weight_decay,\n",
    "        # )\n",
    "        if config.scheduler == \"cos\":\n",
    "            self.scheduler = get_cosine_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=int(self.num_train_steps * config.warmup_epochs),\n",
    "                num_training_steps=int(self.num_train_steps * config.epochs),\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "                self.optimizer,\n",
    "                milestones=[config.epochs - 5, config.epochs - 1],\n",
    "                gamma=0.1,\n",
    "            )\n",
    "        return [self.optimizer]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # \"batch\" is the output of the training data loader.\n",
    "\n",
    "        if self.current_epoch < config.model_freeze_epochs:\n",
    "            for param in self.model.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.model.model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        img, labels = batch\n",
    "        preds = self.model(img, labels)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        # Logs the accuracy per epoch to tensorboard (weighted average over batches)\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=False)\n",
    "\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            self.log(\n",
    "                f\"lr/lr{i}\",\n",
    "                param_group[\"lr\"],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=False,\n",
    "            )\n",
    "\n",
    "        if config.scheduler == \"cos\":\n",
    "            self.scheduler.step()\n",
    "        return loss  # Return tensor to call \".backward\" on\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        img, labels = batch\n",
    "        preds = self.model(img)\n",
    "        loss = self.loss_module(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        # By default logs it per epoch (weighted average over batches)\n",
    "        self.log(\"val_acc\", acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134e9dd-4397-42cd-8f6a-4315346d666a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing train reader...\n",
      "Done.\n",
      "Preparing valid reader...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/luffy/workspace/vpr/data/model_saves/vit_224_v3/lightning_logs\n",
      "/home/luffy/miniconda3/envs/vpr/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/luffy/workspace/vpr/data/model_saves/vit_224_v3 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | Classifier_model | 314 M \n",
      "1 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "314 M     Trainable params\n",
      "0         Non-trainable params\n",
      "314 M     Total params\n",
      "629.570   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004587650299072266,
       "initial": 0,
       "n": 0,
       "ncols": 190,
       "nrows": 49,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004624366760253906,
       "initial": 0,
       "n": 0,
       "ncols": 190,
       "nrows": 49,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e72d9cc782649ab96818ef62bc03b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luffy/miniconda3/envs/vpr/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "config_dict = config.__dict__\n",
    "config_dict = {k: v for k, v in config_dict.items() if not k.startswith(\"__\")}\n",
    "os.makedirs(os.path.join(config.root_dir, \"model_saves\", config.name))\n",
    "with open(\n",
    "    os.path.join(config.root_dir, \"model_saves\", config.name, \"config.yaml\"), \"w\"\n",
    ") as file:\n",
    "    yaml.dump(config_dict, file)\n",
    "\n",
    "train_loader, val_loader, train_dataset = get_dataloaders()\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(config.root_dir, \"model_saves\", config.name),\n",
    "    mode=\"max\",\n",
    "    save_top_k=config.num_models_save,\n",
    "    every_n_epochs=1,\n",
    "    monitor=\"val_acc\",\n",
    "    save_weights_only=True,\n",
    ")\n",
    "model = VPRModule(num_train_steps=len(train_dataset) // config.batch_size)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=checkpoint_callback,\n",
    "    precision=config.precision,\n",
    "    logger=pl.loggers.TensorBoardLogger(\n",
    "        save_dir=os.path.join(config.root_dir, \"model_saves\", config.name)\n",
    "    ),\n",
    ")\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4425f2fb-7ccf-4c09-9f47-100152d2a7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b3f21-a47c-474a-9386-9666f565ed25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
